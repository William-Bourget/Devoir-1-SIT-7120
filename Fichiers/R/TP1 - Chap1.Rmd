---
title: "TP1 - Exercices Chap. 1"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Chapitre 1 - Exercice 1

### a) Montrez que $\hat{b}_1 = \frac{c_1 \hat{\beta}_1}{c_2}$

Avec l'équation donnée dans l'exercice, on remarque facilement qu'il s'agit d'une régression linéaire de la forme $Y_i = \beta_{0} + \beta_{1}x_i + \epsilon_i$ où le $\beta_{0}^{*} = \beta_{0} + \beta_{1} \overline{x}_n$ ce qui résulte à l'équation donnée de l'exercice.
Alors, puisque le $\beta_1$ demeure inchangé, il est possible de partir de l'équation 1.17 du manuel de cours, où on obtient que la valeur $\hat{\beta}_1$ peut être définie comme suit:
$$ \hat{\beta}_1 = \frac{\sum_{i=1}^{n} Y_i (x_i-\overline{x}_n)}{\sum_{i=1}^{n}(x_i-\overline{x}_n)^2}$$
Alors, pour l'estimateur de $\hat{b}_1$, nous obtenons ceci:

\begin{align*}
\hat{b}_1 &= \frac{\sum_{i=1}^{n} \widetilde{Y}_i (\widetilde{x}_i-\overline{\widetilde{x}}_n)}{\sum_{i=1}^{n}(\widetilde{x}_i-\overline{\widetilde{x}}_n)^2} \\
          &= \frac{\sum_{i=1}^{n} c_1 Y_i (x_i-\overline{x}_n) c_2}{\sum_{i=1}^{n}(x_i-\overline{x}_n)^2 c_2^2} \\
          &= \frac{c_1}{c_2} \frac{\sum_{i=1}^{n} Y_i (x_i-\overline{x}_n)}{\sum_{i=1}^{n}(x_i-\overline{x}_n)^2} \\
          &= \frac{c_1}{c_2} \hat{\beta}_1\\
\end{align*}


### b) Montrez que $\widetilde{\sigma}^2 = Var(\widetilde{Y}_i) = c_{1}^2 \sigma^2$ et donc que $\widetilde{s}^2 = c_{1}^2 s_2$ est un estimateur non biaisé de $\widetilde{\sigma}^2$.

\begin{align*}
Var(\widetilde{Y}_i) &= Var(c_1 Y_i) \\
                    &= c_{1}^2 Var(Y_i) \\
                    &= c_{1}^2 \sigma^2 \\
\end{align*}
Alors, on a que
\begin{align*}
\widetilde{s}^2 &= \frac{\sum_{i=1}^{n}(\widetilde{Y}_i - \hat{\widetilde{Y}}_i)^2}{n-p} \\
\widetilde{s}^2 &= \frac{\sum_{i=1}^{n}(c_1 Y_i - (\hat{b}_0 + \hat{b}_1 (\widetilde{x}_i - \overline{\widetilde{x}}_n)))^2}{n-p} \\
\end{align*}


En utilisant le fait que $\hat{b}_1 = \frac{c_1 \hat{\beta}_1}{c_2}$ et $\hat{b}_0 = c_1 \hat{\beta}_0$, on remplace dans l'équation pour obtenir:


\begin{align*}
\widetilde{s}^2 &= \frac{\sum_{i=1}^{n}(c_1 Y_i - (c_1 \hat{\beta}_0 + \frac{c_1 \hat{\beta}_1}{c_2} (\widetilde{x}_i - \overline{\widetilde{x}}_n)))^2}{n-p} \\
                &= \frac{\sum_{i=1}^{n}(c_1 Y_i - (c_1 \hat{\beta}_0 + \frac{c_1 \hat{\beta}_1}{c_2} (x_i - \overline{x}_n) c_2 ))^2}{n-p} \\
                &= c_{1}^2 \frac{\sum_{i=1}^{n}(Y_i - (\hat{\beta}_0 + \hat{\beta}_1(x_i - \overline{x}_n)))^2}{n-p} \\
                &= c_{1}^2 \frac{\sum_{i=1}^{n}(Y_i - \hat{Y}_i)^2}{n-p} \\
                &= c_{1}^2 s^2 \\
\end{align*}




### c) Montrez que si $(\hat{\beta}_{1L}, \hat{\beta}_{1U})$ est un intervalle de confiance à 95% pour $\beta_1$, montrez que $(\frac{c_1}{c_2}\hat{\beta}_{1L}, \frac{c_1}{c_2}\hat{\beta}_{1U})$ est un intervalle de confiance à 95% pour $b_1$.


\begin{align*}
    &Pr \bigg(\hat{b}_1 - t_{\frac{\alpha}{2};n-p} \sqrt{\widetilde{s}^2 (\widetilde{X}'\widetilde{X})^{-1}} \leq b_1 \leq \hat{b}_1 - t_{\frac{\alpha}{2};n-p} \sqrt{\widetilde{s}^2 (\widetilde{X}'\widetilde{X})^{-1}} \bigg) \\
    &= Pr \bigg(\frac{c_1}{c_2}\hat{\beta}_1 - t_{\frac{\alpha}{2};n-p} \sqrt{\frac{c_1^2}{c_2^2}s^2 (X'X)^{-1}} \leq b_1 \leq \frac{c_1}{c_2}\hat{\beta}_1 - t_{\frac{\alpha}{2};n-p} \sqrt{\frac{c_1^2}{c_2^2}s^2 (X'X)^{-1}} \bigg) \\
    &= Pr \Bigg(\frac{c_1}{c_2} \bigg(\hat{\beta}_1 - t_{\frac{\alpha}{2};n-p} \sqrt{s^2 (X'X)^{-1}}\bigg) \leq b_1 \leq \frac{c_1}{c_2} \bigg(\hat{\beta}_1 + t_{\frac{\alpha}{2};n-p} \sqrt{s^2 (X'X)^{-1}}\bigg) \Bigg)\\
    &= Pr \Bigg(\frac{c_1}{c_2} \hat{\beta}_{1L} \leq b_1 \leq \frac{c_1}{c_2} \hat{\beta}_{1U}  \Bigg) \\
\end{align*}


Alors, puisque nous avons que l'intervalle de confiance à 95% de $\beta_1$ est la suivante:
$$ 0.95 = Pr \bigg(\hat{\beta}_1 - t_{\frac{0.05}{2};n-p} \sqrt{s^2 (X'X)^{-1}} \leq \beta_1 \leq \hat{\beta}_1 - t_{\frac{0.05}{2};n-p} \sqrt{s^2 (X'X)^{-1}} \bigg) $$
Nous obtenons que l'intervalle au même niveau de confiance pour $b_1$ étant $(\frac{c_1}{c_2}\hat{\beta}_{1L}, \frac{c_1}{c_2}\hat{\beta}_{1U})$.



### d) Soit $t_1$, la statistique t qui permet de tester l'hypothèse $H_0 : \beta_1 = \beta_{1,0}$. Montrez que la statistique t pour le test $H_0 : b_1 = b_{1,0}$ , où $b_{1,0} = \frac{c1\beta_{1,0}}{c_2}$, sera égale à $t_1$.


\begin{align*}
t &= \frac{\hat{b}_1 - b_{1,0}}{se(\hat{b}_1)} \\
t &= \frac{\frac{c_1 \hat{\beta}_{1,0}}{c_2} - \frac{c_1 \beta_{1,0}}{c_2}}{\sqrt{\widetilde{s}^2 (\widetilde{X}'\widetilde{X})^{-1}}} \\
t &= \frac{\frac{c_1 \hat{\beta}_{1,0}}{c_2} - \frac{c_1 \beta_{1,0}}{c_2}}{\sqrt{\frac{c_1^2}{c_2^2}s^2 (X'X)^{-1}}} \\
t &= \frac{\frac{c_1}{c_2} \hat{\beta}_{1,0} - \frac{c_1}{c_2} \beta_{1,0}}{\frac{c_1}{c_2}\sqrt{s^2 (X'X)^{-1}}} \\
t &= \frac{\hat{\beta}_{1,0} - \beta_{1,0}}{\sqrt{s^2 (X'X)^{-1}}} \\
t &= t_1 \\
\end{align*}


### e) À la lumière des résultats obtenus en (a)-(d), que pouvez-vous dire quant au choix des unités de mesure pour la variable endogène et la variable exogène ?

Comme il est possible de le constater, un changement d'unité de mesure impacte les valeurs des estimations des coefficients de régression. Alors, lors d'un interprétation des coefficients, il est bien important de considérer l'unité de mesure sélectionnée. Par contre, il est possible de voir au numéro (d) que les unités de mesure n'impacteront pas les tests d'hypothèses et en analysant les résultats, il sera possible de rejeter ou non l'hypothèse nulle avec le même niveau de confiance. Alors, dans tous les cas, lorsqu'il y a un changement d'unité de mesure, la régression linéaire s'ajustera en conséquence et nous obtiendrons les mêmes conclusions.




## Chapitre 1 - Exercice 10

### a)
Afin de répondre à la question de l'ingénieur, il est possible de faire un test F de l'importance globale de la régression. Ceci revient donc à tester

$$H_0: \beta_1 = \beta_2 = \beta_3 = 0$$
\begin{center}
$H_1:$ au moins un des coefficients n'est pas 0.
\end{center}

où ceci revient à prendre la statistique F de la table ANOVA et de rejeter $H_0$ lorsque celle-ci prend de grande valeur.


\begin{align*}
F &= \frac{SS_{Reg} / p^{'}}{s^2} \\
F &= \frac{(SS_{TOT} - SS_{Res}) / p^{'}}{s^2} \\
F &= \frac{(SS_{TOT} - s^2 (n-p)) / p^{'}}{s^2} \\
F &= \frac{(\sum_{i=1}^{23} Y_i^2 - n \overline{Y_n}^2 - s^2 (n-p)) / p^{'}}{s^2}\\
F &= \frac{(2850 - (23) (\frac{207}{23})^2 - 7.81 (23-4)) / 3}{7.81} = 35.79 \\
\end{align*}


Alors, on a que
$$Pr[F_{3,19} \geq 35.79] \approx 0$$
Donc on rejete l'hypothèse nulle qu'il n'y a aucun des coefficients qui explique la qualité des stylos. On conclut que les données montrent de l'évidence qu'au moins une des 3 variables exogènes explique une partie de la variable endogène.



###b)
$$E[Y | x_1^{*} = x_1 -1] - E[Y | x_1^{*} = x_1] = E[Y | x_2^{*} = x_2 +1] - E[Y | x_2^{*} = x_2]$$
$$\beta_0 + \beta_1(x_1 - 1) + \beta_2 x_2 + \beta_3 x_3 - (\beta_0 + \beta_1x_1 + \beta_2 x_2 + \beta_3 x_3) = \beta_0 + \beta_1x_1 + \beta_2 (x_2+1) + \beta_3 x_3 - (\beta_0 + \beta_1x_1 + \beta_2 x_2 + \beta_3 x_3)$$

Ceci revient à tester l'hypothèse suivante
$$ -\beta_1 = \beta_2 $$
Alors, on peut substituer $\beta_2$ par $-\beta1$ dans l'équation du modèle complet pour obtenir notre hypothèse nulle.
$$Y = \beta_0 + \beta_1x_1 - \beta_1 x_2 + \beta_3 x_3 + \epsilon$$
$$Y = \beta_0 + \beta_1 (x_1 - x_2) + \beta_3 x_3 + \epsilon$$

Donc nous pouvons tester l'hypothèse nulle selon le principe de somme de carrés résiduelle additionnelle

$$H_0: Y_i = \beta_0 + \beta_1 (x_{i1} - x_{i2}) + \beta_3 x_{i3} + \epsilon_i$$
$$H_1: Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i$$

ce qui revient à comparer l'énoncé (v) de l'exercice contre l'énoncé (i) et nous rejeterons l'hypothèse nulle si la valeur de la statistique F suivante prendra une grande valeur

\begin{align*}
F &= \frac{SS_{Res}^{H_0} - SS_{Res}^{H_1}}{\Delta_{dl} s_{H_1}^2} \\
F &= \frac{s_{H_0}^2 (23-3) - s_{H_1}^2 (23-4)}{1 * s_{H_1}^2} \\
F &= \frac{8.12 (23-3) - 7.81 (23-4)}{1 * 7.81} = 1.7939 \\
\end{align*}

On obtient ensuite que
$$ Pr[F_{1,23-4} \geq 1.7939] = 0.1963$$
Nous obtenons que les données ne montrent pas d'évidence afin de pouvoir rejeter l'hypothèse nulle.


###c)
Ceci revient à tester l'hypothèse suivante:
\begin{align*}
H_0: \beta_3 = 0 \ \ &\Rightarrow  Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i \\
H_1: \beta_3 \neq 0 \ \ &\Rightarrow  Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i \\
\end{align*}

Et il est possible de faire le test en utilisant le principe de somme de carrés résiduelle additionnelle. On compare alors l'équation (iv) à l'équation (i) du numéro.


\begin{align*}
  &= \frac{SS_{Res}^{H_0} - SS_{Res}^{H_1}}{\Delta_{dl} s_{H_1}^2} \\
  &= \frac{s_{H_0}^2 (23-3) - s_{H_1}^2 (23-4)}{1 * s_{H_1}^2} \\
  &= \frac{18.12 (23-3) - 7.81(23-4)}{1 * 7.81} \\
  &= 27.40 \\
\end{align*}


Les données montrent alors de la forte évidence contre $H_0$ puisque $Pr[F_{1,19} \geq 27.40] \approx 0$. On répond alors à l'administrateur que oui, les données montrent de l'évidence que changer le bonus de productivité affectera la qualité de stylos.




## Chapitre 1 - Exercice 12

###a)
$$E[Y_i | m_{iA} = 1, m_{iB} = 0] = E[Y_i | m_{iA} = 0, m_{iB} = 1] = E[Y_i | m_{iA} = 0, m_{iB} = 0]$$
$$\beta_0 + \beta_1x_i + \beta_A + \beta_2 x_i = \beta_0 + \beta_1x_i + \beta_B + \beta_3 x_i = \beta_0 + \beta_1x_i$$
$$\beta_A + \beta_2 x_i = \beta_B + \beta_3 x_i = 0$$
Puisque l'énoncé mentionne que ceci doit être valide peu importe le nombre d'heures consacrés au cours ($x_i$), alors on obtient que le test d'hypothèse revient à tester :
\begin{align*}
H_0&: \beta_A = \beta_B = \beta_2 = \beta_3 =0\\
H_1&: \beta_A \neq \beta_B \neq \beta_2 \neq \beta_3 \neq 0 \\
\end{align*}


###b)
Pour le manuel A, on obtient le résultat suivant:
$$E[Y_i |x_i^{*}=x_i+1, m_{iA} = 1, m_{iB} = 0] - E[Y_i |x_i^{*}=x_i, m_{iA} = 1, m_{iB} = 0]$$
$$ = \beta_0 + \beta_1(x_i+1) + \beta_A + \beta_2(x_i+1) - (\beta_0 + \beta_1x_i + \beta_A + \beta_2x_i)$$
$$ = \beta_1 + \beta_2$$


Pour le manuel B, on obtient le résultat suivant:
$$E[Y_i |x_i^{*}=x_i+1, m_{iA} = 0, m_{iB} = 1] - E[Y_i |x_i^{*}=x_i, m_{iA} = 0, m_{iB} = 1]$$
$$ = \beta_0 + \beta_1(x_i+1) + \beta_B + \beta_3(x_i+1) - (\beta_0 + \beta_1x_i + \beta_B + \beta_3x_i)$$
$$ = \beta_1 + \beta_3$$

Pour le manuel C, on obtient le résultat suivant:
$$E[Y_i |x_i^{*}=x_i+1, m_{iA} = 0, m_{iB} = 0] - E[Y_i |x_i^{*}=x_i, m_{iA} = 0, m_{iB} = 0]$$
$$ = \beta_0 + \beta_1(x_i+1) - (\beta_0 + \beta_1x_i)$$
$$ = \beta_1$$

Afin de tester si les 3 équations ci-dessus sont équivalentes, ceci revient à tester
$$\beta_1 + \beta_2 = \beta_1 + \beta_3 = \beta_1 \ \ \ \Rightarrow \beta_2 = \beta_3 = 0$$

Alors, on doit tester
\begin{align*}
H_0&: \beta_2 = \beta_3 = 0 \\
H_1&: \beta_2 \neq \beta_3 \neq 0 \\
\end{align*}


###c)
Nous voulons alors tester si

$$E[Y|x^{*}_i = 1.25x_i,, m_{iA} = 0, m_{iB} = 0] = E[Y|x^{*}_i = x_i,, m_{iA} = 0, m_{iB} = 1]$$
$$\beta_0 + \beta_11.25x_i = \beta_0 + \beta_1x_i + \beta_B + \beta_3x_i$$
ce qui revient à tester simultanément les 2 conditions suivantes en fonction des termes des coefficients du modèle:
$$ H_0: \beta_B = 0, \ \ \ 0.25\beta_1 = \beta_3$$
$$ H_0: \beta_B \neq 0, \ \ \ 0.25\beta_1 \neq \beta_3$$



###d)
On peut écrire ceci sous la forme suivante:

$$E[Y|x^{*}_i = x_i + 1, m_{iA} = 1, m_{iB} = 0] - E[Y|x^{*}_i = x_i, m_{iA} = 1, m_{iB} = 0] $$
$$ = E[Y|x^{*}_i = x_i+1.5, m_{iA} = 0, m_{iB} = 0] - E[Y|x^{*}_i = x_i, m_{iA} = 0, m_{iB} = 0]$$
$$ \beta_0 + \beta_1(x_i+1) + \beta_A + \beta_2(x_i+1) -(\beta_0 + \beta_1x_i + \beta_A + \beta_2x_i))= \beta_0 + \beta_1(x_i+1.5) - (\beta_0 + \beta_1x_i)$$
$$ \beta_1 + \beta_2= 1.5\beta_1$$
Ce qui revient à tester l'hypothèse suivante:
$$H_0: 0.5\beta_1 = \beta_2 $$
$$H_1: 0.5\beta_1 \neq \beta_2 $$


###e)
En gardant la même notation que dans l'exercice et en respectant les 3 contraintes, on peut écrire le modèle de régression linéaire suivant :

$$Y_i = \beta_0 + \beta_1 (max(x_i;5) - 5) + \beta_C(1-m_{iA}-m_{iB}) + \epsilon_i$$

